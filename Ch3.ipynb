{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION=\"20.04 LTS (Focal Fossa)\"\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "PRETTY_NAME=\"Ubuntu 20.04 LTS\"\n",
      "VERSION_ID=\"20.04\"\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "VERSION_CODENAME=focal\n",
      "UBUNTU_CODENAME=focal\n",
      "__version__='3.0.0'\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!cat /etc/os-release\n",
    "!cat /usr/local/spark/python/pyspark/version.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"version_check\").master(\"local\").getOrCreate() \n",
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_file_dir = \"/home/jovyan/work/Spark-The-Definitive-Guide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 3. 스파크 기능 둘러보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staticDataFrame = spark.read.format(\"csv\")\\\r\n",
      "  .option(\"header\", \"true\")\\\r\n",
      "  .option(\"inferSchema\", \"true\")\\\r\n",
      "  .load(\"/data/retail-data/by-day/*.csv\")\r\n",
      "\r\n",
      "staticDataFrame.createOrReplaceTempView(\"retail_data\")\r\n",
      "staticSchema = staticDataFrame.schema\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.sql.functions import window, column, desc, col\r\n",
      "staticDataFrame\\\r\n",
      "  .selectExpr(\r\n",
      "    \"CustomerId\",\r\n",
      "    \"(UnitPrice * Quantity) as total_cost\",\r\n",
      "    \"InvoiceDate\")\\\r\n",
      "  .groupBy(\r\n",
      "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\r\n",
      "  .sum(\"total_cost\")\\\r\n",
      "  .sort(desc(\"sum(total_cost)\"))\\\r\n",
      "  .show(5)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "streamingDataFrame = spark.readStream\\\r\n",
      "    .schema(staticSchema)\\\r\n",
      "    .option(\"maxFilesPerTrigger\", 1)\\\r\n",
      "    .format(\"csv\")\\\r\n",
      "    .option(\"header\", \"true\")\\\r\n",
      "    .load(\"/data/retail-data/by-day/*.csv\")\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "purchaseByCustomerPerHour = streamingDataFrame\\\r\n",
      "  .selectExpr(\r\n",
      "    \"CustomerId\",\r\n",
      "    \"(UnitPrice * Quantity) as total_cost\",\r\n",
      "    \"InvoiceDate\")\\\r\n",
      "  .groupBy(\r\n",
      "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\r\n",
      "  .sum(\"total_cost\")\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "purchaseByCustomerPerHour.writeStream\\\r\n",
      "    .format(\"memory\")\\\r\n",
      "    .queryName(\"customer_purchases\")\\\r\n",
      "    .outputMode(\"complete\")\\\r\n",
      "    .start()\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "spark.sql(\"\"\"\r\n",
      "  SELECT *\r\n",
      "  FROM customer_purchases\r\n",
      "  ORDER BY `sum(total_cost)` DESC\r\n",
      "  \"\"\")\\\r\n",
      "  .show(5)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.sql.functions import date_format, col\r\n",
      "preppedDataFrame = staticDataFrame\\\r\n",
      "  .na.fill(0)\\\r\n",
      "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\r\n",
      "  .coalesce(5)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "trainDataFrame = preppedDataFrame\\\r\n",
      "  .where(\"InvoiceDate < '2011-07-01'\")\r\n",
      "testDataFrame = preppedDataFrame\\\r\n",
      "  .where(\"InvoiceDate >= '2011-07-01'\")\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.ml.feature import StringIndexer\r\n",
      "indexer = StringIndexer()\\\r\n",
      "  .setInputCol(\"day_of_week\")\\\r\n",
      "  .setOutputCol(\"day_of_week_index\")\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.ml.feature import OneHotEncoder\r\n",
      "encoder = OneHotEncoder()\\\r\n",
      "  .setInputCol(\"day_of_week_index\")\\\r\n",
      "  .setOutputCol(\"day_of_week_encoded\")\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.ml.feature import VectorAssembler\r\n",
      "\r\n",
      "vectorAssembler = VectorAssembler()\\\r\n",
      "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\r\n",
      "  .setOutputCol(\"features\")\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.ml import Pipeline\r\n",
      "\r\n",
      "transformationPipeline = Pipeline()\\\r\n",
      "  .setStages([indexer, encoder, vectorAssembler])\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "fittedPipeline = transformationPipeline.fit(trainDataFrame)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "transformedTraining = fittedPipeline.transform(trainDataFrame)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.ml.clustering import KMeans\r\n",
      "kmeans = KMeans()\\\r\n",
      "  .setK(20)\\\r\n",
      "  .setSeed(1L)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "kmModel = kmeans.fit(transformedTraining)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "transformedTest = fittedPipeline.transform(testDataFrame)\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n",
      "from pyspark.sql import Row\r\n",
      "\r\n",
      "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()\r\n",
      "\r\n",
      "\r\n",
      "# COMMAND ----------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ~/work/Spark-The-Definitive-Guide/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ch3\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://de02416f5426:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>version_check</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdc3601d100>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\") # partitions(데이터 분할) 200(default) -> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(spark_file_dir + \"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticDataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticSchema[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(staticSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|[2011-09-20 00:00...|          71601.44|\n",
      "|      null|[2011-11-14 00:00...|          55316.08|\n",
      "|      null|[2011-11-07 00:00...|          42939.17|\n",
      "|      null|[2011-03-29 00:00...| 33521.39999999998|\n",
      "|      null|[2011-12-08 00:00...|31975.590000000007|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 7.89 ms, sys: 8.65 ms, total: 16.5 ms\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\\\n",
    "  .sort(desc(\"sum(total_cost)\"))\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|[2011-09-20 00:00...|          71601.44|\n",
      "|      null|[2011-11-14 00:00...|          55316.08|\n",
      "|      null|[2011-11-07 00:00...|          42939.17|\n",
      "|      null|[2011-03-29 00:00...| 33521.39999999998|\n",
      "|      null|[2011-12-08 00:00...|31975.590000000007|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 10.3 ms, sys: 10.3 ms, total: 20.7 ms\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\\\n",
    "  .sort(desc(\"sum(total_cost)\"))\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 0 ns, total: 10.1 ms\n",
      "Wall time: 51.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\") # lazy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerId: double, window: struct<start:timestamp,end:timestamp>, sum(total_cost): double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fdc4d5f7ee0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start() # Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerId|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM customer_purchases\n",
    "  ORDER BY `sum(total_cost)` DESC\n",
    "  \"\"\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fdc4d5fc2b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .queryName(\"customer_purchases_2\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start() # Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF() # ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat ~/work/Spark-The-Definitive-Guide/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
